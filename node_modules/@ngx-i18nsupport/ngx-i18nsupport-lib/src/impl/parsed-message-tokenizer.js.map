{"version":3,"file":"parsed-message-tokenizer.js","sourceRoot":"","sources":["../../../../projects/ngx-i18nsupport-lib/src/impl/parsed-message-tokenizer.ts"],"names":[],"mappings":";;AAAA,qCAAqC;AACrC,+BAAuC;AAEvC;;;GAGG;AAEH,SAAS;AACI,QAAA,IAAI,GAAG,MAAM,CAAC;AACd,QAAA,SAAS,GAAG,WAAW,CAAC;AACxB,QAAA,OAAO,GAAG,SAAS,CAAC;AACpB,QAAA,SAAS,GAAG,WAAW,CAAC;AACxB,QAAA,WAAW,GAAG,aAAa,CAAC;AAC5B,QAAA,eAAe,GAAG,iBAAiB,CAAC;AACpC,QAAA,WAAW,GAAG,aAAa,CAAC;AAOzC;IAEY,QAAQ;QACZ,MAAM,KAAK,GAAG,IAAI,QAAQ,EAAE,CAAC;QAC7B,IAAI,SAAS,GAAG,EAAE,CAAC;QACnB,KAAK,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,KAAK,EAAE,IAAI,EAAE,EAAE;YAC9B,IAAI,IAAI,CAAC,IAAI,KAAK,YAAI,IAAI,SAAS,KAAK,EAAE,EAAE;gBACxC,GAAG,CAAC,MAAM,CAAC,YAAI,EAAE,EAAC,IAAI,EAAE,SAAS,EAAC,CAAC,CAAC;gBACpC,SAAS,GAAG,EAAE,CAAC;aAClB;QACL,CAAC,CAAC,CAAC;QACH,KAAK,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,EAAE;YACjB,IAAI,SAAS,KAAK,EAAE,EAAE;gBAClB,GAAG,CAAC,MAAM,CAAC,YAAI,EAAE,EAAC,IAAI,EAAE,SAAS,EAAC,CAAC,CAAC;aACvC;QACJ,CAAC,CAAC,CAAC;QACJ,yGAAyG;QACzG,0EAA0E;QAC1E,KAAK,CAAC,IAAI,CAAC,+CAA+C,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YACvE,MAAM,OAAO,GAAG,wBAAiB,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;YACzE,GAAG,CAAC,MAAM,CAAC,iBAAS,EAAE,EAAC,IAAI,EAAE,KAAK,CAAC,CAAC,CAAC,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC,CAAC;QAChE,CAAC,EAAE,iBAAS,CAAC,CAAC;QACd,sFAAsF;QACtF,KAAK,CAAC,IAAI,CAAC,4CAA4C,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YACpE,MAAM,OAAO,GAAG,wBAAiB,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC;YACzE,GAAG,CAAC,MAAM,CAAC,iBAAS,EAAE,EAAC,IAAI,EAAE,KAAK,CAAC,CAAC,CAAC,EAAE,SAAS,EAAE,OAAO,EAAC,CAAC,CAAC;QAChE,CAAC,EAAE,iBAAS,CAAC,CAAC;QACd,UAAU;QACV,KAAK,CAAC,IAAI,CAAC,6BAA6B,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YACrD,GAAG,CAAC,MAAM,CAAC,eAAO,EAAE,EAAC,IAAI,EAAE,KAAK,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QAC1C,CAAC,EAAE,eAAO,CAAC,CAAC;QACZ,cAAc;QACd,KAAK,CAAC,IAAI,CAAC,cAAc,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YACtC,GAAG,CAAC,MAAM,CAAC,mBAAW,EAAE,EAAC,SAAS,EAAE,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAC,CAAC,CAAC;QACjE,CAAC,EAAE,mBAAW,CAAC,CAAC;QAChB,kBAAkB;QAClB,KAAK,CAAC,IAAI,CAAC,8BAA8B,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YACtD,GAAG,CAAC,MAAM,CAAC,uBAAe,EAAE,EAAC,SAAS,EAAE,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,EAAC,CAAC,CAAC;QACrE,CAAC,EAAE,uBAAe,CAAC,CAAC;QACpB,cAAc;QACd,KAAK,CAAC,IAAI,CAAC,iBAAiB,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YACzC,GAAG,CAAC,MAAM,CAAC,mBAAW,EAAE,EAAC,OAAO,EAAE,KAAK,CAAC,CAAC,CAAC,EAAC,CAAC,CAAC;QACjD,CAAC,EAAE,mBAAW,CAAC,CAAC;QAChB,OAAO;QACP,KAAK,CAAC,IAAI,CAAC,GAAG,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YAC3B,SAAS,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YACtB,GAAG,CAAC,MAAM,EAAE,CAAC;QACjB,CAAC,EAAE,YAAI,CAAC,CAAC;QACT,KAAK,CAAC,IAAI,CAAC,WAAW,EAAE,CAAC,GAAG,EAAE,KAAK,EAAE,EAAE;YACnC,SAAS,IAAI,KAAK,CAAC,CAAC,CAAC,CAAC;YACtB,GAAG,CAAC,MAAM,EAAE,CAAC;QACjB,CAAC,EAAE,YAAI,CAAC,CAAC;QACT,OAAO,KAAK,CAAC;IACjB,CAAC;IAED,QAAQ,CAAC,iBAAyB;QAC9B,MAAM,KAAK,GAAa,IAAI,CAAC,QAAQ,EAAE,CAAC;QACxC,KAAK,CAAC,KAAK,EAAE,CAAC;QACd,KAAK,CAAC,KAAK,CAAC,iBAAiB,CAAC,CAAC;QAC/B,OAAO,KAAK,CAAC,MAAM,EAAE,CAAC;IAC1B,CAAC;CAEJ;AA9DD,sDA8DC","sourcesContent":["import * as Tokenizr from 'tokenizr';\r\nimport {isNullOrUndefined} from 'util';\r\n\r\n/**\r\n * Created by martin on 14.05.2017.\r\n * A tokenizer for normalized messages.\r\n */\r\n\r\n// Tokens\r\nexport const TEXT = 'TEXT';\r\nexport const START_TAG = 'START_TAG';\r\nexport const END_TAG = 'END_TAG';\r\nexport const EMPTY_TAG = 'EMPTY_TAG';\r\nexport const PLACEHOLDER = 'PLACEHOLDER';\r\nexport const ICU_MESSAGE_REF = 'ICU_MESSAGE_REF';\r\nexport const ICU_MESSAGE = 'ICU_MESSAGE';\r\n\r\nexport interface Token {\r\n    type: string;\r\n    value: any;\r\n}\r\n\r\nexport class ParsedMesageTokenizer {\r\n\r\n    private getLexer(): Tokenizr {\r\n        const lexer = new Tokenizr();\r\n        let plaintext = '';\r\n        lexer.before((ctx, match, rule) => {\r\n            if (rule.name !== TEXT && plaintext !== '') {\r\n                ctx.accept(TEXT, {text: plaintext});\r\n                plaintext = '';\r\n            }\r\n        });\r\n        lexer.finish((ctx) => {\r\n            if (plaintext !== '') {\r\n                ctx.accept(TEXT, {text: plaintext});\r\n            }\r\n         });\r\n        // empty tag, there are only a few allowed (see tag-mappings): ['BR', 'HR', 'IMG', 'AREA', 'LINK', 'WBR']\r\n        // format is <name id=\"nr\">, nr ist optional, z.B. <img> oder <img id=\"2\">\r\n        lexer.rule(/<(br|hr|img|area|link|wbr)( id=\"([0-9])*\")?\\>/, (ctx, match) => {\r\n            const idcount = isNullOrUndefined(match[3]) ? 0 : parseInt(match[3], 10);\r\n            ctx.accept(EMPTY_TAG, {name: match[1], idcounter: idcount});\r\n        }, EMPTY_TAG);\r\n        // start tag, Format <name id=\"nr\">, nr ist optional, z.B. <mytag> oder <mytag id=\"2\">\r\n        lexer.rule(/<([a-zA-Z][a-zA-Z-0-9]*)( id=\"([0-9]*)\")?>/, (ctx, match) => {\r\n            const idcount = isNullOrUndefined(match[3]) ? 0 : parseInt(match[3], 10);\r\n            ctx.accept(START_TAG, {name: match[1], idcounter: idcount});\r\n        }, START_TAG);\r\n        // end tag\r\n        lexer.rule(/<\\/([a-zA-Z][a-zA-Z-0-9]*)>/, (ctx, match) => {\r\n            ctx.accept(END_TAG, {name: match[1]});\r\n        }, END_TAG);\r\n        // placeholder\r\n        lexer.rule(/{{([0-9]+)}}/, (ctx, match) => {\r\n            ctx.accept(PLACEHOLDER, {idcounter: parseInt(match[1], 10)});\r\n        }, PLACEHOLDER);\r\n        // icu message ref\r\n        lexer.rule(/<ICU-Message-Ref_([0-9]+)\\/>/, (ctx, match) => {\r\n            ctx.accept(ICU_MESSAGE_REF, {idcounter: parseInt(match[1], 10)});\r\n        }, ICU_MESSAGE_REF);\r\n        // icu message\r\n        lexer.rule(/<ICU-Message\\/>/, (ctx, match) => {\r\n            ctx.accept(ICU_MESSAGE, {message: match[0]});\r\n        }, ICU_MESSAGE);\r\n        // text\r\n        lexer.rule(/./, (ctx, match) => {\r\n            plaintext += match[0];\r\n            ctx.ignore();\r\n        }, TEXT);\r\n        lexer.rule(/[\\t\\r\\n]+/, (ctx, match) => {\r\n            plaintext += match[0];\r\n            ctx.ignore();\r\n        }, TEXT);\r\n        return lexer;\r\n    }\r\n\r\n    tokenize(normalizedMessage: string): Token[] {\r\n        const lexer: Tokenizr = this.getLexer();\r\n        lexer.reset();\r\n        lexer.input(normalizedMessage);\r\n        return lexer.tokens();\r\n    }\r\n\r\n}\r\n"]}